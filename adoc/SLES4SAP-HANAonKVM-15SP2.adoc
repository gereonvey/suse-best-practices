:docinfo:

:localdate:

// Document Variables
:DocumentName: SUSE Best Practices for SAP HANA on KVM
:slesProdVersion: 15 SP2
:suse: SUSE
:SUSEReg: SUSE(R)
:sleAbbr: SLE
:sle: SUSE Linux Enterprise
:sleReg: {SUSEReg} Linux Enterprise
:slesAbbr: SLES
:sles: {sle} Server
:slesReg: {sleReg} Server
:sles4sapAbbr: {slesAbbr} for SAP
:sles4sap: {sles} for SAP Applications
:sles4sapReg: {slesReg} for SAP Applications
:haswell: Intel Xeon Processor E7 v3 (Haswell)
:skylake: 1st Generation Intel Xeon Scalable Processor (Skylake)
:cascadelake: 2nd Generation Intel Xeon Scalable Processor (Cascade Lake)
:launchPadNotes: https://launchpad.support.sap.com/#/notes/


//TODO: Add a support checklist, e.g. for support folks (a shortened version of the guide to help support know what to check)
//TODO: add picture to describe CPU core mappings phys/virt
//TODO: add picture to explain VM Scenarios

= {DocumentName}

{sles4sap} {slesProdVersion}

[[_sec_introduction]]
== Introduction

This best practice document describes how {sles4sap} {slesProdVersion} with KVM should be configured to run SAP HANA for use in production environments.
The setup of the SAP HANA system or other components like HA clusters are beyond the scope of this document.

The following sections describe how to set up and configure the three KVM components required to run SAP HANA on KVM:

* *<<_sec_hypervisor>>* - The host operating system running the Hypervisor directly on the server hardware
* *<<_sec_guest_vm_xml_configuration>>* - The libvirt domain XML description of the guest VM
* *<<_sec_guest_operating_system>>* - The operating system inside the VM where SAP HANA is running

Follow *<<_sec_supported_scenarios_prerequisites>>* and the respective SAP Notes to ensure a supported configuration.
Most of the configuration options are specific to the libvirt package and therefore require modifying the VM guest`'s domain XML file.

[[_sec_definitions]]
=== Definitions

Virtual Machine:: is an emulation of a computer.
Hypervisor:: The software running directly on the physical sever to create and run VMs (Virtual Machines).
Guest OS:: The Operating System running inside the VM (Virtual Machine). 
This is the OS running SAP HANA and therefore the one that should be checked for SAP HANA support as per {launchPadNotes}2235581[SAP Note 2235581 "`SAP HANA: Supported Operating Systems`"] and the https://www.sap.com/dmc/exp/2014-09-02-hana-hardware/enEN/appliances.html["`SAP HANA Hardware Directory`"].
Paravirtualization:: Allows direct communication between the Hypervisor and the VM Guest resulting in a lower overhead and better performance.
libvirt:: A management interface for KVM.
qemu:: The virtual machine emulator, also seen a process on the Hypervisor running the VM.
SI units:: Some commands and configurations use the decimal prefix (for example GB), while other use the binary prefix (for example GiB). In this document we use the binary prefix where possible.

For a general overview of the technical components of the KVM architecture, refer to https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-kvm-intro.html[Section "`Introduction to KVM Virtualization`"] of the Virtualization Guide.

[[_sec_sap_hana_virtualization_scenarios]]
=== SAP HANA Virtualization Scenarios

SAP supports virtualization technologies for SAP HANA usage on a per scenario basis:

Single-VM:: One VM per Hypervisor/physical server for SAP HANA Scale-Up. No other VM or workload is allowed on the same server
Multi-VM:: Multiple VM`'s per Hypervisor/physical server for SAP HANA Scale-Up
Scale-Out:: For a SAP HANA Scale-Out deployment, distributed over multiple VMs on multiple hosts.



[[_sec_supported_scenarios_prerequisites]]
== Supported Scenarios and Prerequisites

*Follow this {DocumentName} - {sles4sap} {slesProdVersion}
        document which describes the steps necessary
        to create a supported SAP HANA on KVM configuration. 
        {sles4sap} must be used for both Hypervisor and Guest.*

Inquiries about scenarios not listed here should be directed to mailto:saphana@suse.com[saphana@suse.com]

[[_sec_supported_scenarios]]
=== Supported Scenarios

At the time of this publication the following configurations are supported for production use:

[[_supported_combinations]]
.Supported Combinations
[cols="1,1,1,1", options="header"]
|===
| CPU Architecture
| SAP HANA scale-up (single VM)
| SAP HANA scale-up (multi VM)
| SAP HANA Scale-out

// |
// {haswell}
// | 
// _Hypervisor:_ {sles4sapAbbr} 12 SP2 
//
// _Guest:_ {sles4sapAbbr} 12 SP1 onwards 
// 
// _Size:_ max. 4 sockets footnote:max4sockets[Maximum 4 sockets using Intel standard chipsets on a single system board, for example Lenovo* x3850, Fujitsu* rx4770 etc.], 2TiB RAM
// |
// no
// |
// no
|
{skylake}
|
_Hypervisor:_ {sles4sapAbbr} 15 SP2 

_Guest:_ {sles4sapAbbr} 15 SP2 onwards 

_Size:_ max. 4 sockets footnote:max4sockets[Maximum 4 sockets using Intel standard chipsets on a single system board, for example Lenovo* x3850, Fujitsu* rx4770 etc.], 3TiB RAM
|
no
|
no
|===


Check the following SAP Notes for the latest details of supported SAP HANA on KVM scenarios.

* {launchPadNotes}2284516[SAP Note 2284516 - "SAP HANA virtualized on SUSE Linux Enterprise Hypervisors"]
* {launchPadNotes}3120786[SAP Note 3120786 - "SAP HANA on SUSE KVM Virtualization with SLES 15 SP2"]

[[_sec_sizing]]
=== Sizing

[[_sec_resources_hypervisor]]
==== Resources for Hypervisor

It is recommended to reserve a minimum of about 8% of the hosts's main memory for the Hypervisor.

The hypervisor will consume CPU capacity, approximately 5% to 10% of the SAPS capacity, depending on the workload characteristics:

* 5% of the SAPS capacity for mainly analytical workloads
* 10% of the SAPS capacity for mainly transactional workloads

It is however *not* required to dedicate CPUs to the hypervisor.

[[_sec_memory_sizing]]
==== Memory Sizing

Since SAP HANA runs inside the VM, it is the RAM size of the VM which has to satisfy the memory requirements from the SAP HANA Memory sizing.

Of course, the memory used by the VM must be smaller than the physical memory of the machine.
It is recommended to reserve at least 8% of the total memory reported by "`/proc/meminfo`" (in the "`MemTotal`" field) for the hypervisor.
This leaves ~ 92% to the VM.

See section <<_sec_memory_backing>> for more details.

[[_sec_cpu_sizing]]
==== CPU Sizing

//TODO: Check CPU Overhead
Some artificial workload tests on {skylake} CPUs have shown an approximately of up to 20% overhead when running SAP HANA on KVM.
Therefore a thorough test of the configuration for the required workload is highly recommended before "`go live`".

There are two main ways to deal with CPU sizing from a sizing perspective:
1. Follow the fixed memory-to-core ratios for SAP HANA as defined by SAP
2. Follow the SAP HANA TDI "`Phase 5`" rules as defined by SAP

Both ways are described in the following sections.

===== Following the fixed memory-to-core ratios for SAP HANA

The certification of the SAP HANA Appliance hardware to be used for KVM prescribes a fixed maximum amount of memory (RAM) which is allowed for each CPU core, also known as "`memory-to-core ratio`". The specific ratio also depends on what workload the system will be used for, that is the Appliance Type: OLTP (Scale-up: SoH/S4H) or OLAP (Scale-up: BWoH/BW4H/DM/SoH/S4H).

The relevant memory-to-core ratio required to size a VM can be easily calculated as follows:

* Go to the https://www.sap.com/dmc/exp/2014-09-02-hana-hardware/enEN/appliances.html["`SAP HANA Certified Hardware Directory`"].
* Select the required SAP HANA Appliance and Appliance Type (for example CPU Architecture "`Intel Skylake SP`" for Appliance Type "`Scale-up: BWoH`").
* Look for the largest certified RAM size for the number of CPU Sockets on the server (for example 3TiB/3072GiB on 4-Socket).
* Look up the number of cores per CPU of this CPU Architecture used in SAP HANA Appliances. The CPU model numbers are listed at: https://www.sap.com/dmc/exp/2014-09-02-hana-hardware/enEN/index.html#details (for example 28).
* Using the above values calculate the total number of cores on the certified Appliance by multiplying number of sockets by number of cores (for example 4x28=112).
* Now divide the Appliance RAM by the total number of cores (not hyperthreads) to give you the "`memory-to-core`" ratio. (for example 3072GiB/112 = approx. 28GiB per core).

Table <<_sap_hana_core_to_memory_ratio_examples>> below has some current examples of SAP HANA memory-to-core ratios.

[[_sap_hana_core_to_memory_ratio_examples]]
.SAP HANA memory-to-core ratio examples
[cols="1,1,1,1,1,1", options="header"]
|===
| CPU Architecture
| Appliance Type
| Max Memory Size
| Sockets
| Cores per Socket
| SAP HANA memory-to-core ratio

// | {haswell} | OLTP | 3TiB / 3072GiB | 4 | 18 | 43GiB/core
// | {haswell} | OLAP | 2TiB / 2048GiB | 4 | 18 | 28GiB/core
| {skylake} | OLTP | 6TiB / 6144GiB | 4 | 28 | 55GiB/core
| {skylake} | OLAP | 3TiB / 3072GiB | 4 | 28 | 28GiB/core
|===


// TODO: Remove or change the following

From your memory requirement, calculate the RAM size the VM needs to be compliant with the appropriate memory-to-core ratio defined by SAP.

* To get the memory per socket, multiply the memory-to-core ratio by the number of cores (not threads) of a single socket in your host
* Divide the memory requirement by the memory per socket, and round the result up to the next full number, and multiply that number by the memory per socket again


.Calculation Example
====
* From an S/4HANA sizing you get a memory requirement for SAP HANA of 4000GiB. 
* Your CPUs have 28 cores per socket. The memory per socket is `28 cores * 55GiB/core = 1540GiB`.   
* Divide your memory requirement `4000GiB / 1540GiB = 2.6` and round this result up to 3. Then multiply `3 * 1540GiB = 4620GiB` 
* 4620GiB is now the memory size to use in the VM configuration as described in <<_sec_memory_backing>>
====


===== Following the SAP HANA TDI "`Phase 5`" rules
** SAP HANA TDI Phase 5 rules allow customers to deviate from the above described SAP HANA memory-to-core sizing ratios in certain scenarios. The KVM implementation must still however adhere to the SUSE Best Practices for SAP HANA on KVM - {sles4sap} {slesProdVersion}. Details on SAP HANA TDI Phase 5 can be found in the following blog from SAP: https://blogs.saphana.com/2017/09/20/tdi-phase-5-new-opportunities-for-cost-optimization-of-sap-hana-hardware/.
** Since SAP HANA TDI Phase 5 rules use SAPS based sizing, SUSE recommends applying the same overhead as measured with SAP HANA on KVM for the respective KVM Version/CPU Architecture. SAPS values for servers can be requested from the respective hardware vendor.


The following SAP HANA sizing documentation should also be useful: 
// * SAP Best Practice "`Sizing Approaches for SAP HANA`": https://websmp203.sap-ag.de/~sapidb/011000358700000050632013E
* https://help.sap.com/viewer/eb3777d5495d46c5b2fa773206bbfb46/2.0.03/en-US/d4a122a7bb57101493e3f5ca08e6b039.html[SAP HANA Master Guide: Sizing SAP HANA] on help.sap.com
* http://sap.com/sizing[General SAP Sizing information]


[[_sec_kvm_hypervisor_version]]
=== KVM Hypervisor Version

The Hypervisor must be configured according to this "`SUSE Best Practices for SAP
          HANA on KVM - {sles4sap} {slesProdVersion}`" guide and fulfill the following minimal requirements:

* {sles4sap} {slesProdVersion} ("`Unlimited Virtual Machines`" subscription)
** kernel (Only major version 5.3, minimum package version 5.3.18-24.24.1)
** libvirt (Only major version 6.0, minimum package version 6.0.0-13.3.1)
** qemu (Only major version 4.2, minimum package version 4.2.1-11.10.1)


[[_sec_hypervisor_hardware]]
=== Hypervisor Hardware

Use SAP HANA certified servers and storage as per SAP HANA Hardware Directory at: https://www.sap.com/dmc/exp/2014-09-02-hana-hardware/enEN/

[[_sec_guest_vm]]
=== Guest VM

The guest VM must:

* Run {sles4sap} 15 SP2 or later.
* Be a {sles} Supported VM Guest as per Section 7.1 "`Supported VM Guests`" of the https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-virt-support.html#virt-support-guests[SUSE Virtualization Guide].
* Comply with KVM limits as per https://www.suse.com/releasenotes/x86_64/SUSE-SLES/15-SP2/#allArch-virtualization-kvm-limits[SUSE Linux Enterprise Server 15 SP2 release notes].
* Fulfill the SAP HANA Hardware and Cloud Measurent Tools (HCMT) storage KPI`'s as per {launchpadnotes}2493172[SAP Note 2493172 "`SAP HANA Hardware and Cloud Measurement Tools`"]. 
  Refer to <<_sec_storage>> for storage configuration details.
* Be configured according to this SUSE Best Practices for SAP HANA on KVM - {sles4sap} {slesProdVersion} document.


[[_sec_hypervisor]]
== Hypervisor

[[_sec_kvm_hypervisor_installation]]
=== KVM Hypervisor Installation

For details refer to Section 6.4 Installation of Virtualization Components of the SUSE Virtualization Guide (https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-vt-installation.html#sec-vt-installation-patterns)

Install the KVM packages using the following Zypper patterns:

----
zypper in -t pattern kvm_server kvm_tools
----

In addition, it is also useful to install the "`lstopo`" tool which is part of the "`hwloc`" package contained inside the "`HPC Module`" for SUSE Linux Enterprise Server.

[[_sec_configure_networking_on_hypervisor]]
=== Configure Networking on Hypervisor

To achieve maximum performance required for productive SAP HANA workloads one of the host networking devices must be assigned directly to the KVM Guest VM.
A Network Interface Card (NIC) including support for the technology that goes under the name of Single Root I/O Virtualization (SR-IOV) is required.
In fact, this guarantees that the overhead in which we would have incurred if using IO Virtualization, is avoided.

In order to check whether such technology is available, assuming that "`17:00.0`" is the address of the NIC on the PCI bus (as visible in the output of the "`lspci`" tool), the following command can be issued:

----
lspci -vs 17:00.0
17:00.0 Ethernet controller: Intel Corporation Ethernet Controller X710 for 10GbE SFP+ (rev 01)
        Subsystem: Intel Corporation Ethernet Converged Network Adapter X710-2
        Flags: bus master, fast devsel, latency 0, IRQ 247, NUMA node 0
        Memory at 9c000000 (64-bit, prefetchable) [size=8M]
        Memory at 9d008000 (64-bit, prefetchable) [size=32K]
        Expansion ROM at 9d680000 [disabled] [size=512K]
        Capabilities: [40] Power Management version 3
        Capabilities: [50] MSI: Enable- Count=1/1 Maskable+ 64bit+
        Capabilities: [70] MSI-X: Enable+ Count=129 Masked-
        Capabilities: [a0] Express Endpoint, MSI 00
        Capabilities: [e0] Vital Product Data
        Capabilities: [100] Advanced Error Reporting
        Capabilities: [140] Device Serial Number d8-ef-c3-ff-ff-fe-fd-3c
        Capabilities: [150] Alternative Routing-ID Interpretation (ARI)
        Capabilities: [160] Single Root I/O Virtualization (SR-IOV)
        Capabilities: [1a0] Transaction Processing Hints
        Capabilities: [1b0] Access Control Services
        Capabilities: [1d0] #19
        Kernel driver in use: i40e
        Kernel modules: i40e
----

The output should contain a line similar to this one: "`Single Root I/O Virtualization (SR-IOV)`".
If such line is not present, it might be the case that SR-IOV needs to be explicitly enabled in the BIOS.

[[_sec_assign_network_port_at_pci_nic_level]]
==== Prepare a Virtual Function (VF) for a Guest VM

After checking that the NIC is SR-IOV capable, the host and the Guest VM should be configured to use one of the available Virtual Functions (VFs) as (one of) the Guest VM's network device(s).
More information about SR-IOV as a technology and on how to properly configure everything that is necessary for it to work well in the general case, can be found in the SUSE Virtualization Guide for SUSE Linux Enterprise Server 15 SP2 (https://documentation.suse.com/sles/15-SP2/single-html/SLES-virtualization).
Specifically, in the section: "Adding SR-IOV Devices" (https://documentation.suse.com/sles/15-SP2/single-html/SLES-virtualization/#sec-libvirt-config-io).

.Enabling PCI Passthrough for the Host Kernel

Make sure that the host kernel boot command line contains these two parameters: "`intel_iommu=on iommu=pt`".

This is done by editing [path]_/etc/defalt/grub_, appending "`intel_iommu=on iommu=pt`" to the string that is assigned to the variable "`GRUB_CMDLINE_LINUX_DEFAULT`" and then running "`update-bootloader`" (more detailed information later in the document).

.Loading and Configuring SR-IOV Host Drivers

Before starting the VM, SR-IOV must be enabled on the desired NIC and the VFs must be created.

Always make sure that the properly SR-IOV capable driver is loaded. For instance, for an "`Intel Corporation Ethernet Controller X710`" NIC, the driver resides in the "`i40e`" kernel module.
It can be loaded with the "`modprobe`" command, but chances are high that it is loaded by default already.

If the SR-IOV capable module is not in use by default and it also fails to load with "`modprobe`", this might mean that another driver, potentially one that is not SR-IOV capable, is the one that is currently loaded.
In which case, it should be removed with the "`rmmod`" command.

Once the proper module is loaded, creating at least one VF happens with the following command (which creates 4 of them):

----
echo 4 > /sys/bus/pci/devices/0000\:17\:00.0/sriov_numvfs
----

Or, assuming that the designated NIC corresponds to the symbolic name of "`eth10`":

----
echo 4 > /sys/class/net/eth10/device/sriov_numvfs
----

The procedure can be automated, in such a way that it happens automatically at boot time, by creating the following systemd unit file [path]_/etc/systemd/system/after.local_:

----
[Unit]
Description=/etc/init.d/after.local Compatibility
After=libvirtd.service
Requires=libvirtd.service
[Service]
Type=oneshot
ExecStart=/etc/init.d/after.local
RemainAfterExit=true

[Install]
WantedBy=multi-user.target
----

And then creating the script [path]_/etc/init.d/after.local_:

----
#! /bin/sh
#
# Copyright (c) 2010 SuSE LINUX Products GmbH, Germany.  All rights reserved.
# ...
echo 4 > /sys/class/net/eth10/device/sriov_numvfs
----

[[_sec_storage_hypervisor]]
=== Storage Configuration on Hypervisor

As with compute resources, the storage used for running SAP HANA must also be SAP certified.
Therefore only the storage from SAP HANA Appliances or SAP HANA Certified Enterprise Storage (https://www.sap.com/dmc/exp/2014-09-02-hana-hardware/enEN/enterprise-storage.html) is supported.
In all cases the SAP HANA storage configuration recommendations from the respective hardware vendor and the SAP HANA Storage Requirements for TDI (https://www.sap.com/documents/2015/03/74cdb554-5a7c-0010-82c7-eda71af511fa.html) should be followed.

As described in <<_sec_configure_networking_on_hypervisor>>, in order to reach the adequate level of performance, the storage drives for actual SAP HANA data are attached to the Guest VM via directly assigning the SAN HBA controller to it.
One difference, though, is that there is no counterpart of SR-IOV commonly available for sorage controllers.
Therefore, a full SAN HBA controller must be dedicated and directly assigned to the Guest VM.

In order to figure out which SAN HBA should be used check the available ones, e.g., with the "`lspci`" command:

----
lspci | grep -i "Fibre Channel"
85:00.0 Fibre Channel: QLogic Corp. ISP2722-based 16/32Gb Fibre Channel to PCIe Adapter (rev 01)
85:00.1 Fibre Channel: QLogic Corp. ISP2722-based 16/32Gb Fibre Channel to PCIe Adapter (rev 01)
ad:00.0 Fibre Channel: QLogic Corp. ISP2722-based 16/32Gb Fibre Channel to PCIe Adapter (rev 01)
ad:00.1 Fibre Channel: QLogic Corp. ISP2722-based 16/32Gb Fibre Channel to PCIe Adapter (rev 01)
----

Of course, the HBAs that are assigned to the Guest VM must not be in use on the host.

The remaining storage configuration details, such as how to add the disks and the HBA controllers to the Guest VM configuration file, as well as what to do with them from inside the Guest VM itself, are available in <<_sec_storage>>.

[[_sec_hypervisor_operating_system_configuration]]
=== Hypervisor Operating System Configuration

[[_sec_vhostmd]]
==== vhostmd
The hypervisor needs to have the vhostmd package installed and the corresponding vhostmd service enabled and started (see also: https://confluence.suse.com/display/SAP/HANAonKVM+vhostmd ).
//TODO: Add a proper, external, link

[[_sec_tuned]]
==== Generic Host Tuning with tuned

In order to apply some less specific, but nevertheless effective, tuning to the host, the "TuneD" (https://tuned-project.org/) tool can be used.

Once installed (the package name is "`tuned`") one of the pre-configured profiles can be selected, or custom one created.
Specifically, the "`virtual-host`" profile should be chosen.
Do not use the "`sap-hana profile`" on the Hypervisor.
This can be achieved with the following commands:

----
zypper in tuned

systemctl enable tuned

systemctl start tuned

tuned-adm profile virtual-host
----

The "`tuned`" daemon should now start automatically at boot time, and it should always load the "`virtual-host`" profile, so there is no need to add any of the above commands in any custom startup script.
If in doubt, it is possible to check whether "`tuned`" is running and what the current profile is with the following command:

----
tuned-adm profile

Available profiles:
- balanced                    - General non-specialized tuned profile
...
- virtual-guest               - Optimize for running inside a virtual guest
- virtual-host                - Optimize for running KVM guests
Current active profile: virtual-host
----

[[_sec_verify_tuned_has_set_cpu_frequency_governor_and_performance_bias]]
===== Power Management Considerations

The CPU frequency governor should be set to "`performance`" to avoid latency issues because of ramping the CPU frequency up and down in response to changes in the system`'s load.
The selected "`tuned`" profile should have done this already, and it is possible to verify that it actually did with the following command:

----
cpupower -c all  frequency-info
----

The governor setting can be verified by looking at the "`current policy`".

Additionally the performance bias setting should also be set to 0 (performance). The performance bias setting can be verified with the following command:

----
cpupower -c all info
----

Modern processors also attempt to save power when they are idle, by switching to a lower power state.
Unfortunately this incurs latency when switching in and out of these states.

In order to avoid that, and achieve better and more consistent the performance, the CPUs should not be allowed to go into too aggressive power saving mode (known as C-states).
It therefore is recommended that only C0 and C1 are used.

This can be enforced by adding the following parameters to the kernel boot command line: "`intel_idle.max_cstate=1`".

In order to double check that only the desired C-states are actually available, the following command can be used:

----
cpupower idle-info
----

The idle state settings can be verified by looking at the "`Available idle states:`" line.


[[_sec_irqbalance]]
==== irqbalance

The irqbalance service should be disabled because it can cause latency issues when the /proc/irq/* files are read.
To disable irqbalance run the following command:

----
systemctl disable irqbalance.service

systemctl stop irqbalance.service
----

[[_sec_no_ksm]]
==== Kernel Samepage Merging (ksm)

Kernel Samepage Merging (KSM, https://www.kernel.org/doc/html/latest/admin-guide/mm/ksm.html ) is of no use, because there is only one single VM, so it should be disabled.
The following command makes sure that it is tuned off and that any sharing and de-duplication activity that may have happened, in case it was enabled, is reverted:

----
echo 2 >  /sys/kernel/mm/ksm/run
----

[[_sec_customize_the_linux_kernel_boot_options]]
==== Customize the Linux Kernel Boot Options

To edit the boot options for the Linux kernel to the following:

. Edit [path]_/etc/defaults/grub_ and add the following boot options to the line "`GRUB_CMDLINE_LINUX_DEFAULT`" (A detailed explanation of these options follows).
+

----
mitigations=auto kvm.nx_huge_pages=off numa_balancing=disable kvm_intel.ple_gap=0 transparent_hugepage=never intel_idle.max_cstate=1 default_hugepagesz=1GB hugepagesz=1GB hugepages=<number of hugepages> intel_iommu=on iommu=pt intremap=no_x2apic_optout
----
+

. Run the following command:
+

----
update-bootloader
----
. Reboot the system:
+

----
reboot
----


[[_sec_technical_explanation_of_the_above_described_configuration_settings]]
==== Technical Explanation of the Above Described Configuration Settings

*Hardware Vulnerabilities Mitigations (mitigations=auto kvm.nx_huge_pages=off)*

Recently, a class of side channel attacks exploiting the branch prediction and the speculative execution capabilities of modern CPUs appeared.
On an affected CPU, these problems cannot be fixed, but their effect and their actual exploitability can be mitigated in software.
However, this has a sometimes non-negligible impact on the performance.

For achieving the best possible security, the software mitigations for these vulnerabilities are being enabled ("`mitigations=auto`") with the only exception of the one that deals with "Machine Check Error Avoidance on Page Size Change (CVE-2018-12207, also known as "iTLB Multiht).

//TODO: We probably want a more generic and little bit more detailed section about mitigations?

*Automatic NUMA Balancing (numa_balancing=disable)*

Automatic NUMA balancing can result in increased system latency and should therefore be disabled.

*KVM PLE-GAP (kvm_intel.ple_gap=0)*

Pause Loop Exit (PLE) is a feature whereby a spinning guest CPU releases the physical CPU until a lock is free.
This is useful in cases where multiple virtual CPUs are using the same physical CPU but causes unnecessary delays when the system is not overcommitted.

*Transparent Hugepages (transparent_hugepage=never)*

Because 1G pages are used for the virtual machine, then there is no additional benefit from having THP enabled.
Disabling it will avoid khugepaged interfering with the virtual machine while it scans for pages to promote to hugepages.

*Processor C-states (intel_idle.max_cstate=1)*

Optimal performance is achieved by limiting the processor to states C0 (normal running state) and C1 (first lower power state).

Note that while there is an exit latency associated with C1 states, it is offset on hyperthread-enabled platforms by the fact sibling cores can borrow resources from sibling cores if they are in the C1 state and some CPUs can boost the CPU frequency higher if siblings are in the C1 state.

*Hugepages (default_hugepagesz=1GB
            hugepagesz=<1GB hugepages=number of hugepages>)*

The use of 1GiB hugepages is to reduce overhead and contention when the guest is updating its page tables.
This requires allocation of 1GiB hugepages on the host.
The number of pages to allocate depends on the memory size of the guest.

1GiB pages are not pageable by the OS, so they always remain in RAM and therefore the "`locked`" definition in libvirt XML files is not required.

It also important to ensure the order of the hugepage options, specifically the "`<number of hugepages>`" option must come after the 1GiB hugepage size definitions.

.Calculating Value
[NOTE]
====
The value for "`number of hugepages`" should be calculated by taking the number GiB`'s of RAM minus approx. 8% for the Hypervisor OS.
For example 3TiB RAM (3072GiB) minus 8% are approx. 2770 hugepages
====

*PCI Passthrough (intel_iommu=on iommu=pt)*

For being able to directly assing host devices (like storage controllers and NIC Virtual Functions), with PCI Passthrough and SR-IOV, the IOMMU must be enabled. On top of that, "`iommu=pt`" makes sure that we setup the devices for the best performance (i.e., passthrough mode)

*Interrupt Remapping (intremap=no_x2apic_optout)*

Interrupt remapping interrupts from devices to be intercepted, validated and routed to a specific CPU (e.g., one where a virtual CPU of the Guest VM that has the device assigned is running).
This parameter makes sure that such feature is always enabled.

[[_sec_guest_vm_xml_configuration]]
== Guest VM XML Configuration

This section describes the modifications required to the libvirt XML definition of the Guest VM.
The libvirt XML may be edited using the following command:

----
virsh edit Guest VM name
----

[[_sec_create_an_initial_guest_vm_xml]]
=== Create an Initial Guest VM XML

Refer to section 9 "`Guest Installation`" of the SUSE Virtualization Guide (https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-kvm-inst.html ).

[[_sec_global_vcpu_configuration]]
=== Global vCPU Configuration

The virtual CPU configuration of the VM Guest should reflect the host CPU configuration as close as possible.
There cannot be any overcommitting of memory or CPU resources.

The CPU model should be set to "`host-passthrough`", and any "`check`" should be disabled.
In addition the "`rdtscp`", "`invtsc`" and "`x2apic`" features are required.

[[_sec_memory_backing]]
=== Memory Backing

Huge pages, sized 1GiB (i.e., 1048576 KiB) must be used for all the Guest VM memory.
This guarantees optimal performance for the Guest VM.

It is necessary that each NUMA cell of the Guest VM have a whole number of huge pages assigned to them (i.e., no fractions of huge pages).
All the NUMA cells should also have the same number of huge pages assigned to them (i.e., the Gues VM memory configuration must be balanced).

Therefore the number of huge pages needs to be dividable by the number of NUMA cells.

For instance, if the host has 3169956100 KiB (i.e., 3 TiB) of memory and we want to leave 91.75% of it to the hypervisor (see <<_sec_memory_sizing>>), and there are 4 NUMA cells, each NUMA cell will have the following number of huge pag>

* (3169956100 * (91.75/100)) / 1048576 / 4 = 693

Which means that, in total, there will have to be the following number of huge pages

* 693 * 4 = 2772

Such number must be passed to the host kernel command line parameter on boot (i.e., "`hugepages=2772`", see <<_sec_technical_explanation_of_the_above_described_configuration_settings>>).

Of course, both the total amount of memory the Guest VM should use, and the fact that such memory must come from 1 GiB huge pages has to be specified in the Guest VM configuration file.

It must also be ensured that the "`memory`" and the "`currentMemory`" element have the same value, in order to disable memory ballooning, which, if enabled, would cause unacceptable latency:

----
<domain type='kvm'>
  <!-- ... -->
  <memory unit='KiB'>2906652672</memory>
  <currentMemory unit='KiB'>2906652672</currentMemory>
  <memoryBacking>
    <hugepages>
      <page size='1048576' unit='KiB'/>
    </hugepages>
    <nosharepages/>
  </memoryBacking>
  <!-- ... -->
</domain>
----

.Memory Unit
[NOTE]
====
The memory unit can be set to GiB to ease the memory computations.
====

[[_sec_vcpu_and_vnuma_topology]]
=== vCPU and vNUMA Topology & Pinning

It is important to map the host topology into the Guest VM, as described below.
In fact, this this allows HANA to spread its own workload threads across many virtual CPUs and NUMA nodes.

For instance, for a 4-socket system, with 28 cores per socket and hyperthreading enabled, the virtual CPU configuration will also have 4 sockets, 28 cores, 2 threads.

Always make sure that, in the Guest VM config file:

* The "`cpu`" "`mode`" attribute is set to "`host-passthrough`".
* The "`cpu`" "`topology`" attribute describes the vCPU NUMA topology of the Guest, as discussed above.
* The attributes of the "`numa`" elements describe which vCPU number ranges belong to which NUMA cell. Care should be taken since these number ranges are not the same as on the host. Additionally:
** The "`cell`" elements describe how much RAM should be distributed per NUMA node. In this 4-node example enter 25% (or 1/4) of the entire Guest VM Memory.
Also refer to <<_sec_memory_backing>> and <<_sec_memory_sizing>> Memory section of this paper for further details.
** Each NUMA cell of the Guest VM has 56 vCPUs
** The distances between the cells are exactly the same as in the physical hardware (as per the output of "`numactl --hardware`").

----
<domain type='kvm'>
  <!-- ... -->
  <cpu mode='host-passthrough' check='none'>
    <topology sockets='4' cores='28' threads='2'/>
    <feature policy='require' name='rdtscp'/>
    <feature policy='require' name='invtsc'/>
    <feature policy='require' name='x2apic'/>
    <numa>
      <cell id='0' cpus='0-55' memory='726663168' unit='KiB'>
        <distances>
          <sibling id='0' value='10'/>
          <sibling id='1' value='21'/>
          <sibling id='2' value='21'/>
          <sibling id='3' value='21'/>
        </distances>
      </cell>
      <cell id='1' cpus='56-111' memory='726663168' unit='KiB'>
        <distances>
          <sibling id='0' value='21'/>
          <sibling id='1' value='10'/>
          <sibling id='2' value='21'/>
          <sibling id='3' value='21'/>
        </distances>
      </cell>
      <cell id='2' cpus='112-167' memory='726663168' unit='KiB'>
        <distances>
          <sibling id='0' value='21'/>
          <sibling id='1' value='21'/>
          <sibling id='2' value='10'/>
          <sibling id='3' value='21'/>
        </distances>
      </cell>
      <cell id='3' cpus='168-223' memory='726663168' unit='KiB'>
        <distances>
          <sibling id='0' value='21'/>
          <sibling id='1' value='21'/>
          <sibling id='2' value='21'/>
          <sibling id='3' value='10'/>
        </distances>
      </cell>
    </numa>
  </cpu>
  <!-- ... -->
</domain>
----

It is also necessary to pin virtual CPUs to physical CPUs, in order to limit the overhead caused by virtual CPUs being moved around physical CPUs by the host scheduler.
Similarly, the memory for each NUMA cell of the Guest VM must be allocated only on the corresponding host NUMA node.

Note that KVM/QEMU uses a static hyperthread sibling CPU APIC ID assignment for virtual CPUs, irrespective of the actual physical CPU APIC ID values on the host.
For example, assuming that the first hyperthread sibling pair is CPU 0 and CPU 112 on the host, you will need to pin that sibling pair to vCPU 0 and vCPU 1.

It is recommended to pin both the various sibling pairs of vCPUs to (the corresponding) sibling pairs of host CPUs.
For instance, vCPU 0 should be pinned to pCPU 0 and 112, and the same applies to vCPU 1.
In fact, as far as both the vCPUs always run on the same physical core, the host scheduler is allowed to execute them on either thread, for example in case only one is free while the other is busy executing host or hypervisor activities.

Using the above information the CPU and memory pinning section of the Guest VM XML can be created.
Below is an example based on the hypothetical example above.

Make sure to take note of the following configuration points:

* The "`vcpu placement`" element lists the total number of vCPUs in the Guest.
* The "`cputune`" element contains the attributes describing the mappings of vCPUs to physical CPUs.
* The "`numatune`" element contains the attributes to describe distribution of RAM across the virtual NUMA nodes (CPU sockets).
** The "`mode`" attribute should be set to "`strict`".
** The appropriate number of nodes should be entered in the "`nodeset`" and "`memnode`" attributes. In this example there are 4 sockets, therefore "`nodeset=0-3`" and cellid 0 to 3.

----
<domain type='kvm'>
  <vcpu placement='static'>224</vcpu>
  <cputune>
    <vcpupin vcpu='0' cpuset='0,112'/>
    <vcpupin vcpu='1' cpuset='0,112'/>
    <vcpupin vcpu='2' cpuset='1,113'/>
    <vcpupin vcpu='3' cpuset='1,113'/>
    <vcpupin vcpu='4' cpuset='2,114'/>
    <vcpupin vcpu='5' cpuset='2,114'/>
    <vcpupin vcpu='6' cpuset='3,115'/>
    <vcpupin vcpu='7' cpuset='3,115'/>
    <vcpupin vcpu='8' cpuset='4,116'/>
    <vcpupin vcpu='9' cpuset='4,116'/>
    <vcpupin vcpu='10' cpuset='5,117'/>
    <vcpupin vcpu='11' cpuset='5,117'/>
    <!-- output abbreviated -->
    <vcpupin vcpu='218' cpuset='109,221'/>
    <vcpupin vcpu='219' cpuset='109,221'/>
    <vcpupin vcpu='220' cpuset='110,222'/>
    <vcpupin vcpu='221' cpuset='110,222'/>
    <vcpupin vcpu='222' cpuset='111,223'/>
    <vcpupin vcpu='223' cpuset='111,223'/>
  </cputune>
  <numatune>
    <memory mode='strict' nodeset='0-3'/>
    <memnode cellid='0' mode='strict' nodeset='0'/>
    <memnode cellid='1' mode='strict' nodeset='1'/>
    <memnode cellid='2' mode='strict' nodeset='2'/>
    <memnode cellid='3' mode='strict' nodeset='3'/>
  </numatune>
  <!-- ... -->
</domain>
----

The following script generates a section of the domain configuration according the described specifications:

----
#!/usr/bin/env bash
NUM_VCPU=$(ls -d /sys/devices/system/cpu/cpu[0-9]* | wc -l)
echo "  <vcpu placement='static'>${NUM_VCPU}</vcpu>"
echo "  <cputune>"
THREAD_PAIRS="$(cat /sys/devices/system/cpu/cpu*/topology/core_cpus_list | sort -n | uniq )"
VCPU=0
for THREAD_PAIR in ${THREAD_PAIRS}; do
  for i in 1 2; do
    echo "    <vcpupin vcpu='${VCPU}' cpuset='${THREAD_PAIR}'/>"
    VCPU=$(( VCPU + 1 ))
  done
done
echo "  </cputune>"
----

The following commands can be used to determine the CPU details on the Hypervisor host:

----
lscpu --extended=CPU,SOCKET,CORE

lstopo-no-graphics
----

It is not necessary to isolate the Guest VM's "`iothreads`", nor to statically reserve any host CPU to either them or any other kind of host activity.

[[_sec_vhostmd_guest]]
=== vhostmd Device

The vhostmd device is passed to the VM so that the "`vm-dump-metrics command`" can retrieve metrics about the hypervisor provided by vhostmd.
You can use either a vbd disk or a virtio-serial device (preferred) to set this up (see {launchPadNotes}1522993[SAP Note 1522993 - "`Linux: SAP on SUSE KVM - Kernel-based Virtual Machine`"] for details).


[[_sec_clocks_timers]]
=== Clocks and Timers

Make sure that the clock timers are set up as follows, in the Guest VM config file:

----
<domain type='kvm'>
  <!-- ... -->
  <clock offset='utc'>
    <timer name='rtc' tickpolicy='catchup'/>
    <timer name='pit' tickpolicy='delay'/>
    <timer name='hpet' present='no'/>
  </clock>
  <!-- ... -->
</domain>
----

[[_sec_virtio_rng]]
=== Virtio Random Number Generator (RNG) Device

The host /dev/random file should be passed through to QEMU as a source of entropy using the virtio RNG device:

----
 <domain type='kvm'>
  <!-- ... -->
  <devices>
    <!-- ... -->
    <rng model='virtio'>
      <backend model='random'>/dev/urandom</backend>
    </rng>
    <!-- ... -->
  </devices>
  <!-- ... -->
</domain>
----

[[_sec_network]]
=== Networking

One of the Virtual Functions prepared in <<_sec_configure_networking_on_hypervisor>> must be added to the Guest VM as (on of) its network adapter.
This can be done putting the following in the Guest VM configuration file:

----
 <domain type='kvm'>
  <!-- ... -->
  <devices>
    <!-- ... -->
    <interface type='hostdev' managed='yes'>
      <mac address='52:54:00:7f:12:fb'/>
      <driver name='vfio'/>
      <source>
        <address type='pci' domain='0x0000' bus='0x17' slot='0x02' function='0x0'/>
      </source>
    </interface>
    <!-- ... -->
  </devices>
  <!-- ... -->
</domain>
----

Of course, the various properties (e.g., "`domain`", "`bus`", etc.) of the "`address`" element should contain the proper values for pointing at the desired device (check with "`lspci`").

[[_sec_storage]]
=== Storage

Since storage controller passthrough is used (see <<_sec_storage_hypervisor>>), any LVM (Logical Volume Manager) and Multipathing configuration should, if wanted, be made inside the Guest VM, always following the storage layout recommendations from the appropriate hardware vendor.

Ultimately the storage for SAP HANA must be able to fulfill the SAP HANA HWCCT requirements from within the VM.
For details on HWCCT and the required storage KPI`'s refer to SAP Note 1943937 "`Hardware Configuration Check Tool - Central Note`" (https://launchpad.support.sap.com/notes/1943937) and SAP Note 2501817 - HWCCT 1.0 (≥220) (https://launchpad.support.sap.com/notes/2501817).

Network Attached Storage has not been tested with SAP HANA on KVM.
If there is a requirement for this, please contact mailto:saphana@suse.com[].

[[_sec_storage_configuration_for_operating_system_volumes]]
==== Storage Configuration for Operating System Volumes

The performance of storage where the Operating System is installed is not critical for the performance of SAP HANA, and therefore any KVM supported storage may be used to deploy the Operating system itself.

For instance:

----
<domain type='kvm'>
  <!-- ... -->
  <devices>
    <!-- ... -->
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='native'/>
      <source dev='/dev/disk/by-id/wwn-0x600000e00d29000000293db000520000'/>
      <target dev='vda' bus='virtio'/>
    </disk>
    <!-- ... -->
  </devices>
  <!-- ... -->
</domain>
----

Of course, the "`dev`" attribute of the "`source`" element should contain the appropriate path.

[[_sec_storage_configuration_for_sap_hana_volumes]]
==== Storage Configuration for SAP HANA Volumes

The Guest VM XML configuration must be based on the underlying storage configuration on the Hypervisor (see section <<_sec_storage_hypervisor>>)

Since the storage for HANA ("`/data`", "`/log/`" and "`/shared`" volumes) is performance critical, it is recommended to take advantage of an SAN HBA that is passed through to the Guest VM.


Note that it is not possible to only use one function of the adapter, and both must always be attached to the Guest VM.
An example Guest VM configuration with storage passthrough configured would look like this (adjust the domain, bus, slot and function attributes of the "`address`" elements to match the adapter you chose):

----
<domain type='kvm'>
  <!-- ... -->
  <devices>
    <!-- ... -->
    <hostdev mode='subsystem' type='pci' managed='yes'>
      <source>
        <address domain='0x0000' bus='0x85' slot='0x00' function='0x0'/>
      </source>
    </hostdev>
    <hostdev mode='subsystem' type='pci' managed='yes'>
      <source>
        <address domain='0x0000' bus='0x85' slot='0x00' function='0x1'/>
      </source>
    </hostdev>
    <!-- ... -->
  </devices>
  <!-- ... -->
</domain>
----

More details about how to directly assign PCI devices to a Guest VM are described in https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-libvirt-config-virsh.html#sec-libvirt-config-pci-virsh )


[[_sec_guest_operating_system]]
== Guest Operating System

[[_sec_install_sles_for_sap_inside_the_guest_vm]]
=== Install SUSE Linux Enterprise Server for SAP Applications Inside the Guest VM

Refer to the https://documentation.suse.com/sles-sap/15-SP2/[SUSE Guide "`SUSE Linux Enterprise Server for SAP Applications 15].
          

[[_sec_guest_operating_system_configuration_for_sap_hana]]
=== Guest Operating System Configuration for SAP HANA

Install and configure {sles4sap} {slesProdVersion} and SAP HANA as described in: 

* {launchPadNotes}1944799[SAP Note 1944799 - "`SAP HANA Guidelines for SLES Operating System Installation`"]
* {launchPadNotes}2205917[SAP Note 2684254 - "`SAP HANA DB: Recommended OS settings for SLES 15 / SLES for SAP Applications 15`"]

[[_sec_customizing_linux_cmdline_guest]]
==== Customizing the Linux Kernel Parameters of the Guest

Just like the hypervisor host, the VM also needs special kernel parameters to be set. 
To edit the boot options for the Linux kernel to the following:

. Edit [path]_/etc/defaults/grub_ and add the following boot options to the line "`GRUB_CMDLINE_LINUX_DEFAULT`".
+

----
mitigations=auto kvm.nx_huge_pages=off intremap=no_x2apic_optout
----
+

A detailed explanation of these parameters has been given in <<_sec_technical_explanation_of_the_above_described_configuration_settings>>.

[[_sec_enabling_host_monitoring_guest]]
==== Enabling Host Monitoring

The VM needs to have the `vm-dump-metrics` package installed, which dumps the metrics provided by the `vhostmd` service running on the hypervisor. This enables SAP HANA can collect data about the hypervisor. 
{launchPadNotes}1522993[SAP Note 1522993 - "`Linux: SAP on SUSE KVM - Kernel-based Virtual Machine`"] describes how to set up the virtual devices for `vhostmd` and how to configure it.
When using a virtual disk for `vhostmd`, the virtual disk device must be world-readable, which is ensured with the boot time configuration below.


[[_sec_configuring_guest_at_boot_time]]
==== Configuring the Guest at Boot Time

The folling settings have to be configured at boot time of the VM. 
In order to persist these configurations it is recommended to put the commands provided below into a script which is executed as part of the boot process.

===== Disable `irqbalance` 

The irqbalance service should be disabled because it can cause latency issues when the /proc/irq/* files are read.
To disable irqbalance run the following command:

----
systemctl disable irqbalance.service
systemctl stop irqbalance.service
----

===== Activate and configure `sapconf` or `saptune`

The following parameters need to set in `sapconf` version 5.

----
GOVERNOR=performance
PERF_BIAS=performance
MIN_PERF_PCT=100
FORCE_LATENCY=5 
----

If you use `sapconf` version 4 or `saptune`, please configure it accordingly.
When using `sapconf` version 4, start the `tuned` service and activate the TuneD profile `sapconf`. 
When using `sapconf` version 5, stop and disable the `tuned` service and instead enable and start the `sapconf` service.


===== Activate and configure `haltpoll`



----
POLL_NS=800000
GROW_START=200000
modprobe cpuidle-haltpoll
echo $POLL_NS > /sys/module/haltpoll/parameters/guest_halt_poll_ns
echo $GROW_START > /sys/module/haltpoll/parameters/guest_halt_poll_grow_start
----

===== Set the clock source

The clock source has to be set to `tsc`.

----
echo tsc > /sys/devices/system/clocksource/clocksource0/current_clocksource
----

===== Disable Kernel Same Page Merging

Kernel Same Page Merging (KSM) needs to be disabled, just like on the hypervisor (see <<_sec_no_ksm>>).

----
echo 2 >/sys/kernel/mm/ksm/run
----

===== Automatic configuration at boot time
The following script is provided as an example for a script implementing above recommendations, to be executed at boot time of the VM.


.Script
----
#!/usr/bin/env bash
#
# Configure KVM Guest for SAP HANA
#
 
POLL_NS=800000
GROW_START=200000
 
# disable irqbalance 
systemctl disable --now irqbalance
 
# Check sapconf version installed, runs setup accordingly
MAJOR_VER=$(rpm -qa | awk -F "-" '/sapconf/ {split($2,maj,"."); print maj[1]}')
case ${MAJOR_VER} in
    4)
      echo "SAPCONF V4 detected."
      systemctl enable --now tuned
      tuned-adm profile sapconf
      ;;
    5)
      echo "SAPCONF V5 detected."
      systemctl disable --now tuned
      systemctl enable --now sapconf
      ;;
    *)
      echo "ERROR: Unsupported SAPCONF ver. (${MAJOR_VER}) detected. Exiting."
      exit 1
      ;;
esac
 
modprobe cpuidle-haltpoll
echo $POLL_NS > /sys/module/haltpoll/parameters/guest_halt_poll_ns
echo $GROW_START > /sys/module/haltpoll/parameters/guest_halt_poll_grow_start
 
# Set clocksource to tsc
echo tsc > /sys/devices/system/clocksource/clocksource0/current_clocksource
 
# disable Kernel Samepage Merging
echo 2 >/sys/kernel/mm/ksm/run
# 2: disable it, but make sure you also purify everything with fire!
 
# fix access to vhostmd device, so that SIDadm can read it
# see function setup_vhostmd_guest_device() in qacss-schwifty-common
 
# the vhostmd device has exactly 256 blocks, try to catch that from /proc/partitions
VHOSTMD_DEVICE=$(grep "   256 " /proc/partitions | awk '{print $4}' )
if [ -n "$VHOSTMD_DEVICE" ]; then
  chmod o+r /dev/"$VHOSTMD_DEVICE"
else
  echo "Missing vhostmd device, please check you XML file."
fi
----



[[_sec_guest_operating_system_storage_configuration_for_sap_hana_volumes]]
=== Guest Operating System Storage Configuration for SAP HANA Volumes

* Follow the storage layout recommendations from the appropriate hardware vendors.
* Only use LVM (Logical Volume Manager) inside the VM for SAP HANA. Nested LVM is not to be used.




[[_sec_administration]]
== Administration

For a full explanation of administration commands, refer to official SUSE Virtualization documentation such as:

* Section 10 "`Basic VM Guest Management`" and others in the SUSE Virtualization Guide for SUSE Linux Enterprise Server 15 (https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-libvirt-managing.html)
* SUSE Virtualization Best Practices for SUSE Linux Enterprise Server 15 (https://documentation.suse.com/sles/15-SP2/html/SLES-all/article-vt-best-practices.html)


[[_sec_useful_commands_on_the_hypervisor]]
=== Useful Commands on the Hypervisor

Checking kernel boot options used

----
cat /proc/cmdline
----

Checking hugepage status (This command can also be used to monitor the progress of hugepage allocation during VM start)

----
cat /proc/meminfo | grep Huge
----

List all VM Guest domains configured on Hypervisor

----
virsh list --all
----

Start a VM (Note: VM start times can take some minutes on larger RAM systems, check progress with `/proc/meminfo | grep Huge`

----
virsh start VM/Guest Domain name
----

Shut down a VM

----
virsh shutdown VM/Guest Domain name
----

Location of VM Guest configuration files

----
/etc/libvirt/qemu
----

Location of VM Log files

----
/var/log/libvirt/qemu
----

[[_sec_useful_commands_inside_the_vm_guest]]
=== Useful Commands Inside the VM Guest

Checking L3 cache has been enabled in the guest

----
lscpu | grep L3
----

Validating Guest and Host CPU Topology

----
lscpu
----

[[_sec_examples]]
== Examples


[[_sec_example_guest_vm_xml]]
=== Example Guest VM XML 

.XML Configuration Example
[CAUTION]
====
The XML file below is only an *example* showing the key configurations based on the about command outputs to assist in understanding how to configure the XML.
The actual XML configuration must be based on your respective hardware configuration and VM requirements.
====

Points of interest in this example (refer to the detailed sections of SUSE Best Practices for SAP HANA on KVM - {sles4sap} {slesProdVersion} for a full explanation):

* Memory
** The Hypervisor has 3TiB RAM (or 3072GiB), of which 2772GiB has been allocated as 1GB Hugepages and therefore 2772GiB is the max VM size in this case
** 2772GiB = 2906652672KiB
** In the "`numa`" section memory is split evenly over the 4 NUMA nodes (CPU sockets)
* CPU Pinning
** Note the alternating CPU pinning on the Hypervisor, see <<_sec_vcpu_and_vnuma_topology>> section for details
** Note the topology of the Guest VM mirrors the one of the Hypervisor (4x28 CPU cores)
* Network I/O
** Virtual functions of the physical network interface card have been added as PCI devices
* Storage I/O
** A single SAN HBA is passed through to the VM as `hostdev` device (one for each function/port)
** See <<_sec_storage>> section for details
* "` rng model='virtio' `", for details see section <<_sec_virtio_rng>>
* qemu:commandline elements to describe CPU attributes, for details see section <<_sec_global_vcpu_configuration>>


The following VM definition is for a VM configured to consume a 4-socket server with 3TiB of main memory, . a taken from our actual validation machine.
Please note that this file is abridged for clarity (denoted by a "`[...]`" mark). 

----
# cat /etc/libvirt/qemu/SUSEKVM.xml
!--
WARNING: THIS IS AN AUTO-GENERATED FILE. CHANGES TO IT ARE LIKELY TO BE
OVERWRITTEN AND LOST. Changes to this xml configuration should be made using:
  virsh edit SUSEKVM
or other application using the libvirt API.
--

<domain type='kvm'>
  <name>kvmvm11</name>
  <uuid>f529e0b0-93cc-4e83-87dc-65cb9922336d</uuid>
  <description>kvmvm11</description>
  <metadata>
    <libosinfo:libosinfo xmlns:libosinfo="http://libosinfo.org/xmlns/libvirt/domain/1.0">
      <libosinfo:os id="http://suse.com/sle/15.2"/>
    </libosinfo:libosinfo>
  </metadata>
  <memory unit='KiB'>2906652672</memory>
  <currentMemory unit='KiB'>2906652672</currentMemory>
  <memoryBacking>
    <hugepages>
      <page size='1048576' unit='KiB'/>
    </hugepages>
    <nosharepages/>
  </memoryBacking>
  <vcpu placement='static'>224</vcpu>
  <cputune>
    <vcpupin vcpu='0' cpuset='0,112'/>
    <vcpupin vcpu='1' cpuset='0,112'/>
    <vcpupin vcpu='2' cpuset='1,113'/>
    <vcpupin vcpu='3' cpuset='1,113'/>
    <vcpupin vcpu='4' cpuset='2,114'/>
    <vcpupin vcpu='5' cpuset='2,114'/>
    <vcpupin vcpu='6' cpuset='3,115'/>
    <vcpupin vcpu='7' cpuset='3,115'/>
    <vcpupin vcpu='8' cpuset='4,116'/>
    <vcpupin vcpu='9' cpuset='4,116'/>
    <vcpupin vcpu='10' cpuset='5,117'/>
    <vcpupin vcpu='11' cpuset='5,117'/>
[...]
    <vcpupin vcpu='214' cpuset='107,219'/>
    <vcpupin vcpu='215' cpuset='107,219'/>
    <vcpupin vcpu='216' cpuset='108,220'/>
    <vcpupin vcpu='217' cpuset='108,220'/>
    <vcpupin vcpu='218' cpuset='109,221'/>
    <vcpupin vcpu='219' cpuset='109,221'/>
    <vcpupin vcpu='220' cpuset='110,222'/>
    <vcpupin vcpu='221' cpuset='110,222'/>
    <vcpupin vcpu='222' cpuset='111,223'/>
    <vcpupin vcpu='223' cpuset='111,223'/>
  </cputune>
  <numatune>
    <memory mode='strict' nodeset='0-3'/>
    <memnode cellid='0' mode='strict' nodeset='0'/>
    <memnode cellid='1' mode='strict' nodeset='1'/>
    <memnode cellid='2' mode='strict' nodeset='2'/>
    <memnode cellid='3' mode='strict' nodeset='3'/>
  </numatune>
  <resource>
    <partition>/machine</partition>
  </resource>
  <os>
    <type arch='x86_64' machine='pc-q35-4.2'>hvm</type>
    <loader readonly='yes' type='pflash'>/usr/share/qemu/ovmf-x86_64-smm-ms-code.bin</loader>
    <nvram>/var/lib/libvirt/qemu/nvram/kvmvm12_VARS.fd</nvram>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
    <kvm>
      <hint-dedicated state='on'/>
    </kvm>
    <vmport state='off'/>
  </features>
  <cpu mode='host-passthrough' check='none'>
    <topology sockets='4' cores='28' threads='2'/>
    <feature policy='require' name='rdtscp'/>
    <feature policy='require' name='invtsc'/>
    <feature policy='require' name='x2apic'/>
    <numa>
      <cell id='0' cpus='0-55' memory='726663168' unit='KiB'>
        <distances>
          <sibling id='0' value='10'/>
          <sibling id='1' value='21'/>
          <sibling id='2' value='21'/>
          <sibling id='3' value='21'/>
        </distances>
      </cell>
      <cell id='1' cpus='56-111' memory='726663168' unit='KiB'>
        <distances>
          <sibling id='0' value='21'/>
          <sibling id='1' value='10'/>
          <sibling id='2' value='21'/>
          <sibling id='3' value='21'/>
        </distances>
      </cell>
      <cell id='2' cpus='112-167' memory='726663168' unit='KiB'>
        <distances>
          <sibling id='0' value='21'/>
          <sibling id='1' value='21'/>
          <sibling id='2' value='10'/>
          <sibling id='3' value='21'/>
        </distances>
      </cell>
      <cell id='3' cpus='168-223' memory='726663168' unit='KiB'>
        <distances>
          <sibling id='0' value='21'/>
          <sibling id='1' value='21'/>
          <sibling id='2' value='21'/>
          <sibling id='3' value='10'/>
        </distances>
      </cell>
    </numa>
  </cpu>
  <clock offset='utc'>
    <timer name='rtc' tickpolicy='catchup'/>
    <timer name='pit' tickpolicy='delay'/>
    <timer name='hpet' present='no'/>
  </clock>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>destroy</on_crash>
  <pm>
    <suspend-to-mem enabled='no'/>
    <suspend-to-disk enabled='no'/>
  </pm>
  <devices>
    <emulator>/usr/bin/qemu-system-x86_64</emulator>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='native'/>
      <source dev='/dev/disk/by-id/wwn-0x600000e00d29000000293db000520000'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x04' slot='0x00' function='0x0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/dev/shm/vhostmd0'/>
      <target dev='vdx' bus='virtio'/>
      <readonly/>
      <address type='pci' domain='0x0000' bus='0x0b' slot='0x00' function='0x0'/>
    </disk>
    <controller type='usb' index='0' model='qemu-xhci' ports='15'>
      <address type='pci' domain='0x0000' bus='0x02' slot='0x00' function='0x0'/>
    </controller>
    <controller type='sata' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x1f' function='0x2'/>
    </controller>
    <controller type='pci' index='0' model='pcie-root'/>
    <controller type='pci' index='1' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='1' port='0x10'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0' multifunction='on'/>
    </controller>
    <controller type='pci' index='2' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='2' port='0x11'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x1'/>
    </controller>
    <controller type='pci' index='3' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='3' port='0x12'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x2'/>
    </controller>
    <controller type='pci' index='4' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='4' port='0x13'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x3'/>
    </controller>
    <controller type='pci' index='5' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='5' port='0x14'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x4'/>
    </controller>
    <controller type='pci' index='6' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='6' port='0x15'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x5'/>
    </controller>
    <controller type='pci' index='7' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='7' port='0x16'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x6'/>
    </controller>
    <controller type='pci' index='8' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='8' port='0x17'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x7'/>
    </controller>
    <controller type='pci' index='9' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='9' port='0x18'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0' multifunction='on'/>
    </controller>
    <controller type='pci' index='10' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='10' port='0x19'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x1'/>
    </controller>
    <controller type='pci' index='11' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='11' port='0x1a'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x2'/>
    </controller>
    <controller type='pci' index='12' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='12' port='0x1b'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x3'/>
    </controller>
    <controller type='pci' index='13' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='13' port='0x1c'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x4'/>
    </controller>
    <controller type='pci' index='14' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='14' port='0x1d'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x5'/>
    </controller>
    <controller type='virtio-serial' index='0'>
      <address type='pci' domain='0x0000' bus='0x03' slot='0x00' function='0x0'/>
    </controller>
    <interface type='direct'>
      <mac address='0c:fd:37:92:dc:99'/>
      <source dev='eth11' mode='vepa'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x01' slot='0x00' function='0x0'/>
    </interface>
    <interface type='hostdev' managed='yes'>
      <mac address='52:54:00:7f:12:fb'/>
      <driver name='vfio'/>
      <source>
        <address type='pci' domain='0x0000' bus='0x17' slot='0x02' function='0x0'/>
      </source>
      <address type='pci' domain='0x0000' bus='0x0c' slot='0x00' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target type='isa-serial' port='0'>
        <model name='isa-serial'/>
      </target>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <channel type='unix'>
      <target type='virtio' name='org.qemu.guest_agent.0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
    <channel type='spicevmc'>
      <target type='virtio' name='com.redhat.spice.0'/>
      <address type='virtio-serial' controller='0' bus='0' port='2'/>
    </channel>
    <input type='tablet' bus='usb'>
      <address type='usb' bus='0' port='1'/>
    </input>
    <input type='mouse' bus='ps2'/>
    <input type='keyboard' bus='ps2'/>
    <graphics type='spice' autoport='yes'>
      <listen type='address'/>
      <image compression='off'/>
    </graphics>
    <sound model='ich9'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x1b' function='0x0'/>
    </sound>
    <video>
      <model type='qxl' ram='65536' vram='65536' vgamem='16384' heads='1' primary='yes'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x0'/>
    </video>
    <!-- SAN 2-port HBA passthrough configuration -->
    <hostdev mode='subsystem' type='pci' managed='yes'>
      <source>
        <address domain='0x0000' bus='0x85' slot='0x00' function='0x0'/>
      </source>
      <address type='pci' domain='0x0000' bus='0x0d' slot='0x00' function='0x0'/>
    </hostdev>
    <hostdev mode='subsystem' type='pci' managed='yes'>
      <source>
        <address domain='0x0000' bus='0x85' slot='0x00' function='0x1'/>
      </source>
      <address type='pci' domain='0x0000' bus='0x0e' slot='0x00' function='0x0'/>
    </hostdev>
    <redirdev bus='usb' type='spicevmc'>
      <address type='usb' bus='0' port='2'/>
    </redirdev>
    <redirdev bus='usb' type='spicevmc'>
      <address type='usb' bus='0' port='3'/>
    </redirdev>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x05' slot='0x00' function='0x0'/>
    </memballoon>
    <rng model='virtio'>
      <backend model='random'>/dev/urandom</backend>
      <address type='pci' domain='0x0000' bus='0x06' slot='0x00' function='0x0'/>
    </rng>
  </devices>
</domain>
----

[[_sec_additional_information]]
== Additional Information

[[_sec_resources]]
=== Resources

* https://documentation.suse.com/sbp/all/[SUSE Best Practices]
* https://documentation.suse.com/sles/15-SP2/html/SLES-all/book-virt.html[SUSE Virtualization Guide for SUSE Linux Enterprise Server 15]
* {launchpadnotes}3120786[SAP Note 3120786 - SAP HANA on SUSE KVM Virtualization with SLES 15 SP2]
* {launchPadNotes}2284516[SAP Note 2284516 - "SAP HANA virtualized on SUSE Linux Enterprise Hypervisors"]
* {launchPadNotes}1944799[SAP Note 1944799 - "`SAP HANA Guidelines for SLES Operating System Installation`"]
* {launchPadNotes}2205917[SAP Note 2684254 - "`SAP HANA DB: Recommended OS settings for SLES 15 / SLES for SAP`"] 
* {launchPadNotes}1522993[SAP Note 1522993 - "`Linux: SAP on SUSE KVM - Kernel-based Virtual Machine`"]


[[_sec_feedback]]
=== Feedback

Several feedback channels are available:

Bugs and Enhancement Requests::
For services and support options available for your product, refer to http://www.suse.com/support/.

To report bugs for a product component, go to https://scc.suse.com/support/ requests, log in, and select Submit New SR (Service Request).

Report Documentation Bug::
To report errors or suggest enhancements for a certain document, use the mailto:Report Documentation Bug[] feature at the right side of each section in the online documentation.
Provide a concise description of the problem and refer to the respective section number and page (or URL).

Mail::
For feedback on the documentation of this product, you can also send a mail to mailto:doc-team@suse.com[].
Make sure to include the document title, the product version and the publication date of the documentation.

[[_sec_version_history]]
=== Version History

[cols="1,1,2,3", options="header"]
|===
| Version | Publication Date | Author(s) | Comment

| 0.5 | Nov 2021 | Dario Faggioli, Gereon Vey | Initial draft

|===

[[_sec_legal_notice]]
== Legal Notice

Copyright 2006–2021 SUSE LLC and contributors.
All rights reserved. 

Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation License, Version 1.2 or (at your option) version 1.3; with the Invariant Section being this copyright notice and license.
A copy of the license version 1.2 is included in the section entitled "`GNU Free Documentation License`".

SUSE, the SUSE logo and YaST are registered trademarks of SUSE LLC in the United States and other countries.
For SUSE trademarks, see http://www.suse.com/company/legal/.
Linux is a registered trademark of Linus Torvalds.
All other names or trademarks mentioned in this document may be trademarks or registered trademarks of their respective owners.

This article is part of a series of documents called "SUSE Best Practices". The individual documents in the series were contributed voluntarily by SUSE's employees and by third parties.

All information found in this book has been compiled with utmost attention to detail.
However, this does not guarantee complete accuracy. 

Therefore, we need to specifically state that neither SUSE LLC, its affiliates, the authors, nor the translators may be held liable for possible errors or the consequences thereof.
Below we draw your attention to the license under which the articles are published.

